package ai

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"net/http"
	"os"
	"strings"
	"time"
)

type OllamaProvider struct {
	BaseURL   string
	Model     string
	Endpoints []string
}

func NewOllamaProvider() *OllamaProvider {
	// Candidates to probe
	candidates := []string{}

	// 1. Env Var (User Override)
	if env := os.Getenv("OLLAMA_HOST"); env != "" {
		candidates = append(candidates, env)
	}

	// 2. Standard Localhost
	candidates = append(candidates, "http://localhost:11434")

	// 3. WSL2 Specifics
	// 3a. User specified hardcoded fallback
	candidates = append(candidates, "http://172.17.224.1:11434")

	// 3b. Try to detect via /etc/resolv.conf (WSL Host)
	if wslHost, err := detectWSLHost(); err == nil && wslHost != "" {
		candidates = append(candidates, fmt.Sprintf("http://%s:11434", wslHost))
	}

	return &OllamaProvider{
		BaseURL:   "", // Will be resolved in IsAvailable
		Model:     "llama3",
		Endpoints: candidates,
	}
}

func (p *OllamaProvider) Name() string {
	if p.BaseURL != "" {
		return fmt.Sprintf("Ollama (Local @ %s)", p.BaseURL)
	}
	return "Ollama (Local)"
}

func (p *OllamaProvider) IsAvailable(ctx context.Context) bool {
	// If already resolved, just ping
	if p.BaseURL != "" {
		return p.ping(ctx, p.BaseURL)
	}

	// Otherwise, probe all candidates
	for _, endpoint := range p.Endpoints {
		if p.ping(ctx, endpoint) {
			p.BaseURL = endpoint
			return true
		}
	}
	return false
}

func (p *OllamaProvider) ping(ctx context.Context, url string) bool {
	client := http.Client{Timeout: 500 * time.Millisecond} // Fast fail
	req, _ := http.NewRequestWithContext(ctx, "GET", url+"/api/tags", nil)
	resp, err := client.Do(req)
	if err != nil {
		return false
	}
	defer resp.Body.Close()
	return resp.StatusCode == 200
}

func detectWSLHost() (string, error) {
	// Read /etc/resolv.conf
	content, err := os.ReadFile("/etc/resolv.conf")
	if err != nil {
		return "", err
	}

	// Look for nameserver x.x.x.x
	for _, line := range strings.Split(string(content), "\n") {
		if strings.HasPrefix(line, "nameserver") {
			fields := strings.Fields(line)
			if len(fields) >= 2 {
				return fields[1], nil
			}
		}
	}
	return "", nil
}

type ollamaRequest struct {
	Model  string `json:"model"`
	Prompt string `json:"prompt"`
	System string `json:"system"`
	Stream bool   `json:"stream"`
	Format string `json:"format"` // json mode
}

type ollamaTagsResponse struct {
	Models []struct {
		Name string `json:"name"`
	} `json:"models"`
}

func (p *OllamaProvider) resolveModel(ctx context.Context) error {
	// If user asked strict model, keeps it (not implemented yet, defaults to llama3)
	// But here we want ease of use.

	client := http.Client{Timeout: 2 * time.Second}
	req, _ := http.NewRequestWithContext(ctx, "GET", p.BaseURL+"/api/tags", nil)
	resp, err := client.Do(req)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	var tags ollamaTagsResponse
	if err := json.NewDecoder(resp.Body).Decode(&tags); err != nil {
		return err
	}

	if len(tags.Models) > 0 {
		// Pick the first one. Most users have "latest" or "llama3" as default.
		p.Model = tags.Models[0].Name
		return nil
	}

	return fmt.Errorf("no models found in Ollama")
}

type ollamaResponse struct {
	Response string `json:"response"`
}

func (p *OllamaProvider) GenerateGovernance(ctx context.Context, description string) (*GovernanceBlueprint, error) {
	// Auto-detect model if possible
	if err := p.resolveModel(ctx); err != nil {
		return nil, fmt.Errorf("ollama model check failed: %w", err)
	}

	reqBody := ollamaRequest{
		Model:  p.Model,
		Prompt: fmt.Sprintf("Project Description: %s", description),
		System: SystemPrompt,
		Stream: false,
		Format: "json",
	}

	jsonBody, _ := json.Marshal(reqBody)
	client := http.Client{Timeout: 300 * time.Second} // Generative tasks take time (CPU inference)

	resp, err := client.Post(p.BaseURL+"/api/generate", "application/json", bytes.NewBuffer(jsonBody))
	if err != nil {
		return nil, fmt.Errorf("failed to call ollama: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != 200 {
		return nil, fmt.Errorf("ollama returned status: %d", resp.StatusCode)
	}

	var oResp ollamaResponse
	if err := json.NewDecoder(resp.Body).Decode(&oResp); err != nil {
		return nil, fmt.Errorf("failed to decode ollama response: %w", err)
	}

	// Parse the inner JSON generated by LLM
	var blueprint GovernanceBlueprint
	// Clean potential markdown fences if model disobeys
	cleanJSON := strings.TrimPrefix(oResp.Response, "```json")
	cleanJSON = strings.TrimSuffix(cleanJSON, "```")

	if err := json.Unmarshal([]byte(cleanJSON), &blueprint); err != nil {
		return nil, fmt.Errorf("failed to parse blueprint json: %w. Raw: %s", err, oResp.Response)
	}

	return &blueprint, nil
}
